{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **DISCLAIMER/AVISO**\n",
        "\n",
        "O script a seguir é uma junção do script fornecido pela monitora Glória Carvalho da disciplina de Métodos e Técnicas de Pesquisa II, com adições e trocas a partir de sugestões do Gemini, que reformulou algumas linhas de acordo com o nosso arquivo. Portanto, que fique claro que esse script não foi escrito por nenhum membro desse grupo."
      ],
      "metadata": {
        "id": "jNYpjP4GoSSm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **(1) Instalar e carregar as bibliotecas**"
      ],
      "metadata": {
        "id": "67Z_uJixoPNf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oa5qK_pzH8Jf"
      },
      "outputs": [],
      "source": [
        "!pip install corextopic\n",
        "\n",
        "import nltk\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import corextopic\n",
        "from corextopic import corextopic as ct"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **(2) Abrindo e lendo a planilha**"
      ],
      "metadata": {
        "id": "Ub5BMUqpo6ka"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_excel('/content/LDA_final.xlsx')\n",
        "\n",
        "caminho = '/content/LDA_final.xlsx'\n",
        "\n",
        "nomes_colunas = [\"key\", \"jornal\", \"transcrito\", \"tópico 1\", \"tópico 2\", \"tópico 3\", \"tópico 4\", \"tópico 5\"]\n",
        "\n",
        "df = pd.read_excel(caminho, names=nomes_colunas)"
      ],
      "metadata": {
        "id": "Wtan2tWIJQqL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# (2.1) Exibir os resultados\n",
        "\n",
        "df.head()"
      ],
      "metadata": {
        "id": "xSMuTye1pXfr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **(3) Criando um dicionário a partir das palavras das matérias e inserindo elas em um novo dataframe**"
      ],
      "metadata": {
        "id": "CwDiQHVXpdLW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rows = []"
      ],
      "metadata": {
        "id": "bHJ6glmxptz7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# (3.1) Itera sobre cada linha da coluna \"transcrito\" e adiciona o texto a uma nova lista de dicionários\n",
        "\n",
        "for text in df['transcrito']:\n",
        "    rows.append({\"text\": text})"
      ],
      "metadata": {
        "id": "ElufO6vKpwGc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# (3.2) Cria um novo DataFrame a partir dessa lista de dicionários\n",
        "\n",
        "new_df = pd.DataFrame(rows)"
      ],
      "metadata": {
        "id": "fxEHz9Nlp4TZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# (3.3) Exibe o tamanho do novo DataFrame (para certificar de que todas as células textuais foram realocadas)\n",
        "\n",
        "print(len(new_df))"
      ],
      "metadata": {
        "id": "TSfx5Qy9qAdK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# (3.4) Exibe o novo DataFrame para verificar o resultado\n",
        "\n",
        "print(new_df.head())"
      ],
      "metadata": {
        "id": "-O6vwCGIqlCr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **(4) Transformando as palavras em vetores**"
      ],
      "metadata": {
        "id": "wwo8hBa4qwHh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# (4.1) Cria um objeto TfidVectorizer com as especificações desejadas\n",
        "\n",
        "vectorizer = TfidfVectorizer(\n",
        "    max_df=75,\n",
        "    min_df=2,\n",
        "    max_features=None,\n",
        "    ngram_range=(1, 2),\n",
        "    norm=None,\n",
        "    binary=True,\n",
        "    use_idf=False,\n",
        "    sublinear_tf=False\n",
        ")"
      ],
      "metadata": {
        "id": "wwkcfuNyq94G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# (4.2) Ajusta o vetorizador aos textos da coluna 'text'\n",
        "\n",
        "vectorizer = vectorizer.fit(new_df['text'])"
      ],
      "metadata": {
        "id": "vxUrzk0orKLd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# (4.3) Transforma os textos em uma matriz TF-IDF\n",
        "\n",
        "tfidf = vectorizer.transform(new_df['text'])"
      ],
      "metadata": {
        "id": "rJ68CziprbjY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# (4.4) Obtém o vocabulário utilizado no modelo TF-IDF\n",
        "\n",
        "vocab = vectorizer.get_feature_names_out()"
      ],
      "metadata": {
        "id": "EAFvyhjoreeT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# (4.5) Exibe o tamanho do vocabulário\n",
        "\n",
        "print(len(vocab))"
      ],
      "metadata": {
        "id": "NtPHr9S8rfGw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **(5) Criando as palavras âncoras**"
      ],
      "metadata": {
        "id": "2mA6ZSDrrsSP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "anchors = []\n",
        "model = ct.Corex(n_hidden=4, seed=42)\n",
        "model = model.fit(\n",
        "    tfidf,\n",
        "    words=vocab\n",
        ")"
      ],
      "metadata": {
        "id": "plQ46d06J34z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **(6) Treinando o modelo sem supervisão (modelagem não supervisionada)**"
      ],
      "metadata": {
        "id": "_rRVPi-Ar_tk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i, topic_ngrams in enumerate(model.get_topics(n_words=10)):\n",
        "    topic_ngrams = [ngram[0] for ngram in topic_ngrams if ngram[1] > 0]\n",
        "    print(\"Topic #{}: {}\".format(i+1, \", \".join(topic_ngrams)))"
      ],
      "metadata": {
        "id": "k3YAov05KKck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **(7) Fazendo a modelagem semi-supervisionada (usando as palavras âncoras)**"
      ],
      "metadata": {
        "id": "da7ZqNI5sL-c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Modelagem apenas do Estadão*"
      ],
      "metadata": {
        "id": "Hpqqycx7scqI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# (7.1) Filtrando o DataFrame para incluir apenas textos do Estadão\n",
        "\n",
        "df_estadao = df[df['jornal'] == 'Estadão']\n",
        "\n",
        "# (7.2) Repetir os procedimentos do (3.1) até o (4.5)\n",
        "\n",
        "rows = []\n",
        "\n",
        "for text in df_estadao['transcrito']:\n",
        "    rows.append({\"text\": text})\n",
        "\n",
        "new_df = pd.DataFrame(rows)\n",
        "\n",
        "print(len(new_df))\n",
        "\n",
        "print(new_df.head())\n",
        "\n",
        "vectorizer = TfidfVectorizer(\n",
        "    max_df=75,\n",
        "    min_df=2, # Changed min_df from 0.5 to 2\n",
        "    max_features=None,\n",
        "    ngram_range=(1, 2),\n",
        "    norm=None,\n",
        "    binary=True,\n",
        "    use_idf=False,\n",
        "    sublinear_tf=False\n",
        ")\n",
        "\n",
        "vectorizer = vectorizer.fit(new_df['text'])\n",
        "tfidf = vectorizer.transform(new_df['text'])\n",
        "vocab = vectorizer.get_feature_names_out()\n",
        "\n",
        "print(len(vocab))\n",
        "\n",
        "# (7.3) Criando as palavras âncoras e direcionando o algoritmo\n",
        "\n",
        "anchors = [\n",
        "    [\"greve\"],\n",
        "    [\"decreto\"],\n",
        "    [\"policia\"],\n",
        "    [\"jose_serra\"],\n",
        "     [\"reitoria\"]]\n",
        "\n",
        "anchors = [\n",
        "    [a for a in topic if a in vocab]\n",
        "    for topic in anchors\n",
        "]\n",
        "\n",
        "model = ct.Corex(n_hidden=5, seed=42)\n",
        "model = model.fit(\n",
        "    tfidf,\n",
        "    words=vocab,\n",
        "    anchors=anchors,\n",
        "    anchor_strength=3\n",
        ")\n",
        "\n",
        "# (7.4) Roda a modelagem utilizando as palavras âncoras estipuladas\n",
        "\n",
        "for i, topic_ngrams in enumerate(model.get_topics(n_words=10)):\n",
        "    topic_ngrams = [ngram[0] for ngram in topic_ngrams if ngram[1] > 0]\n",
        "    print(\"Topic #{}: {}\".format(i+1, \", \".join(topic_ngrams)))\n"
      ],
      "metadata": {
        "id": "ayHyTWX2ZjXM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Modelagem apenas da Folha de São Paulo*"
      ],
      "metadata": {
        "id": "t4j9uXCttjXJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# (7.5) Repetir o mesmo processo, mas agora para a FSP\n",
        "\n",
        "df_fsp = df[df['jornal'] == 'FSP']\n",
        "\n",
        "rows = []\n",
        "for text in df_fsp['transcrito']:\n",
        "    rows.append({\"text\": text})\n",
        "\n",
        "new_df = pd.DataFrame(rows)\n",
        "\n",
        "print(len(new_df))\n",
        "\n",
        "print(new_df.head())\n",
        "\n",
        "vectorizer = TfidfVectorizer(\n",
        "    max_df=75,\n",
        "    min_df=2,\n",
        "    max_features=None,\n",
        "    ngram_range=(1, 2),\n",
        "    norm=None,\n",
        "    binary=True,\n",
        "    use_idf=False,\n",
        "    sublinear_tf=False\n",
        ")\n",
        "\n",
        "vectorizer = vectorizer.fit(new_df['text'])\n",
        "\n",
        "tfidf = vectorizer.transform(new_df['text'])\n",
        "\n",
        "vocab = vectorizer.get_feature_names_out()\n",
        "\n",
        "print(len(vocab))\n",
        "\n",
        "anchors = [\n",
        "    [\"reitoria\"],\n",
        "    [\"greve\"],\n",
        "    [\"decreto\"],\n",
        "    [\"policia\"],\n",
        "    [\"jose_serra\"]]\n",
        "\n",
        "anchors = [\n",
        "    [a for a in topic if a in vocab]\n",
        "    for topic in anchors\n",
        "]\n",
        "\n",
        "model = ct.Corex(n_hidden=5, seed=42)\n",
        "model = model.fit(\n",
        "    tfidf,\n",
        "    words=vocab,\n",
        "    anchors=anchors,\n",
        "    anchor_strength=3\n",
        ")\n",
        "\n",
        "for i, topic_ngrams in enumerate(model.get_topics(n_words=10)):\n",
        "    topic_ngrams = [ngram[0] for ngram in topic_ngrams if ngram[1] > 0]\n",
        "    print(\"Topic #{}: {}\".format(i+1, \", \".join(topic_ngrams)))\n"
      ],
      "metadata": {
        "id": "9EppSpjaof3B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Modelagem apenas do Jornal da USP*"
      ],
      "metadata": {
        "id": "CrQRNOZ7uArn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# (7.6) Repetir o mesmo processo, mas agora para o Jornal da USP\n",
        "\n",
        "df_Jornal_USP = df[df['jornal'] == 'Jornal_USP']\n",
        "\n",
        "rows = []\n",
        "\n",
        "for text in df_Jornal_USP['transcrito']:\n",
        "    rows.append({\"text\": text})\n",
        "\n",
        "new_df = pd.DataFrame(rows)\n",
        "\n",
        "print(len(new_df))\n",
        "\n",
        "print(new_df.head())\n",
        "\n",
        "vectorizer = TfidfVectorizer(\n",
        "    max_df=75,\n",
        "    min_df=2,\n",
        "    max_features=None,\n",
        "    ngram_range=(1, 2),\n",
        "    norm=None,\n",
        "    binary=True,\n",
        "    use_idf=False,\n",
        "    sublinear_tf=False\n",
        ")\n",
        "\n",
        "vectorizer = vectorizer.fit(new_df['text'])\n",
        "\n",
        "tfidf = vectorizer.transform(new_df['text'])\n",
        "\n",
        "vocab = vectorizer.get_feature_names_out()\n",
        "\n",
        "print(len(vocab))\n",
        "\n",
        "anchors = [\n",
        "    [\"reitoria\"],\n",
        "    [\"decreto\"],\n",
        "    [\"congregacao\"],\n",
        "    [\"estudantes\"],\n",
        "    [\"patrimonio\"]]\n",
        "\n",
        "anchors = [\n",
        "    [a for a in topic if a in vocab]\n",
        "    for topic in anchors\n",
        "]\n",
        "\n",
        "model = ct.Corex(n_hidden=5, seed=42)\n",
        "model = model.fit(\n",
        "    tfidf,\n",
        "    words=vocab,\n",
        "    anchors=anchors,\n",
        "    anchor_strength=3\n",
        ")\n",
        "\n",
        "for i, topic_ngrams in enumerate(model.get_topics(n_words=10)):\n",
        "    topic_ngrams = [ngram[0] for ngram in topic_ngrams if ngram[1] > 0]\n",
        "    print(\"Topic #{}: {}\".format(i+1, \", \".join(topic_ngrams)))\n"
      ],
      "metadata": {
        "id": "3oOGMT0dp2Wc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Modelagem apenas do Jornal do Campus*"
      ],
      "metadata": {
        "id": "mtKhZcuIukTz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# (7.7) Repetir o mesmo processo, mas agora para o Jornal do Campus\n",
        "import nltk\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import corextopic\n",
        "from corextopic import corextopic as ct\n",
        "\n",
        "df_JC = df[df['jornal'] == 'JC']\n",
        "\n",
        "rows = []\n",
        "\n",
        "for text in df_JC['transcrito']:\n",
        "    rows.append({\"text\": text})\n",
        "\n",
        "new_df = pd.DataFrame(rows)\n",
        "\n",
        "print(len(new_df))\n",
        "\n",
        "print(new_df.head())\n",
        "\n",
        "vectorizer = TfidfVectorizer(\n",
        "    max_df=75,\n",
        "    min_df=2,\n",
        "    max_features=None,\n",
        "    ngram_range=(1, 2),\n",
        "    norm=None,\n",
        "    binary=True,\n",
        "    use_idf=False,\n",
        "    sublinear_tf=False\n",
        ")\n",
        "\n",
        "vectorizer = vectorizer.fit(new_df['text'])\n",
        "\n",
        "tfidf = vectorizer.transform(new_df['text'])\n",
        "\n",
        "vocab = vectorizer.get_feature_names_out()\n",
        "\n",
        "print(len(vocab))\n",
        "\n",
        "anchors = [\n",
        "    [\"reitoria\"],\n",
        "    [\"autonomia\"],\n",
        "    [\"estudante\"],\n",
        "    [\"sintusp\"],\n",
        "    [\"adusp\"],]\n",
        "\n",
        "anchors = [\n",
        "    [a for a in topic if a in vocab]\n",
        "    for topic in anchors\n",
        "]\n",
        "\n",
        "model = ct.Corex(n_hidden=5, seed=42)\n",
        "model = model.fit(\n",
        "    tfidf,\n",
        "    words=vocab,\n",
        "    anchors=anchors,\n",
        "    anchor_strength=3\n",
        ")\n",
        "\n",
        "for i, topic_ngrams in enumerate(model.get_topics(n_words=10)):\n",
        "    topic_ngrams = [ngram[0] for ngram in topic_ngrams if ngram[1] > 0]\n",
        "    print(\"Topic #{}: {}\".format(i+1, \", \".join(topic_ngrams)))"
      ],
      "metadata": {
        "id": "avZsHNkZqZxL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}